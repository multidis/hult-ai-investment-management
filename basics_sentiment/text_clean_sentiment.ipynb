{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e3d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning and sentiment analysis.\n",
    "import html\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc4a6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals\\nhttps://t.co/A7qRIiddKK\\n#powertools'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text example with newlines \"\\n\"\n",
    "text = 'Prime Deals\\nhttps://t.co/A7qRIiddKK\\n#powertools'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69908b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals https://t.co/A7qRIiddKK #powertools'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substitute newlines with spaces (regular expression match and substitute)\n",
    "re.sub('\\n+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c660d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals  #powertools'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove URLs (links are not for sentiment analysis)\n",
    "re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e848d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a smiley face ðŸ˜‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze text content, not encoded emoji etc.\n",
    "semoji = 'This is a smiley face \\U0001f602'\n",
    "semoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7031f96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a smiley face '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove non-ASCII characters to leave only the text\n",
    "# CAUTION: if needeng to analyze multilingual text, modify this as in the answer below for example\n",
    "# https://stackoverflow.com/questions/51784964/remove-emojis-from-multilingual-unicode-text/51785357#51785357\n",
    "semoji.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570780a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "# example text of some Tweet: more complex html encoding\n",
    "text = '\\\"You don\\u2019t need a car to enjoy Melbourne\\u2019s food &amp; culture scene \\u2013 but you don\\u2019t want to ride everywhere, either. If only there was something in between. Oh, wait \\u2013 there is: an eBike. https://t.co/iEzvKd0LZu'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbc3f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food & culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. \n"
     ]
    }
   ],
   "source": [
    "# this string has special symbols encoded (\"escaped characters\")\n",
    "# construct the original text (note for example: \"&amp\" becoming \"&\")\n",
    "# rememberimng to also remove new lines first\n",
    "text_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text)))\n",
    "print(text_unesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b91ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string cleanup function: collect the steps so far\n",
    "def text_cleanup_init(s):\n",
    "    s_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text)))\n",
    "    s_noemoji = s_unesc.encode('ascii', 'ignore').decode('ascii')\n",
    "    # normalize to lowercase\n",
    "    return s_noemoji.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f761507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e953ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"you dont need a car to enjoy melbournes food & culture scene  but you dont want to ride everywhere, either. if only there was something in between. oh, wait  there is: an ebike. \n"
     ]
    }
   ],
   "source": [
    "text_clean_init = text_cleanup_init(text)\n",
    "print(text_clean_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9be29a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'need',\n",
       " 'a',\n",
       " 'car',\n",
       " 'to',\n",
       " 'enjoy',\n",
       " 'melbournes',\n",
       " 'food',\n",
       " '&',\n",
       " 'culture',\n",
       " 'scene',\n",
       " 'but',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'everywhere',\n",
       " ',',\n",
       " 'either',\n",
       " '.',\n",
       " 'if',\n",
       " 'only',\n",
       " 'there',\n",
       " 'was',\n",
       " 'something',\n",
       " 'in',\n",
       " 'between',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'wait',\n",
       " 'there',\n",
       " 'is',\n",
       " ':',\n",
       " 'an',\n",
       " 'ebike',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the string (NLTK tools)\n",
    "word_tokens = word_tokenize(text_clean_init)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aae593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nor', 'hasn', \"you're\", 'will', 'between', 'or', \"haven't\", 'we', 'weren', 'there', 'from', \"wasn't\", 'it', 'because', 'through', 'up', 'isn', \"aren't\", 'other', \"don't\", 'is', 'are', 'herself', 'they', 'an', 'once', 'who', 'me', 't', 'all', 'too', \"isn't\", 'this', 'your', \"shan't\", 'only', 're', 'what', 'himself', 'those', 'did', 'the', \"couldn't\", 'but', 'to', 'any', 'below', 'does', 'why', 'above', 'few', 'its', 'were', 'our', 'during', 'again', \"she's\", 'been', 'll', 'if', 'had', \"weren't\", 'where', 'needn', \"needn't\", 'which', 'i', 'he', 'won', 'am', \"you'd\", 'themselves', 'mustn', 'against', 'him', 'don', 'when', 'doesn', \"hadn't\", \"mustn't\", 'under', 'each', 'y', \"didn't\", 'her', 've', 'having', 'shan', 'couldn', \"doesn't\", 'both', 'ain', \"shouldn't\", 'by', 'with', 'ours', 'being', 'after', 's', 'should', 'most', 'haven', 'just', \"wouldn't\", 'myself', 'yourselves', 'than', \"should've\", 'that', 'doing', \"it's\", 'wasn', 'yours', 'them', 'their', 'about', 'do', 'yourself', 'now', 'while', 'you', \"you've\", 'shouldn', \"that'll\", 'out', 'off', 'not', 'aren', 'and', 'same', 'his', \"you'll\", 'hadn', 'some', 'over', 'ourselves', 'was', 'before', 'mightn', 'into', 'theirs', 'further', 'very', 'as', 'in', 'o', 'has', 'ma', 'here', 'a', 'these', 'be', 'have', 'she', \"mightn't\", 'at', 'down', 'then', 'd', 'how', 'wouldn', 'on', 'more', 'can', \"won't\", 'whom', 'no', 'own', 'm', 'of', 'itself', 'for', 'until', \"hasn't\", 'didn', 'my', 'hers', 'so', 'such'}\n"
     ]
    }
   ],
   "source": [
    "# we want to remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3f00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# and also any punctuation marks\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "751495a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dont',\n",
       " 'need',\n",
       " 'car',\n",
       " 'enjoy',\n",
       " 'melbournes',\n",
       " 'food',\n",
       " 'culture',\n",
       " 'scene',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'ride',\n",
       " 'everywhere',\n",
       " 'either',\n",
       " 'something',\n",
       " 'oh',\n",
       " 'wait',\n",
       " 'ebike']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words and punctuation, also any non-alphanumeric strings (not words)\n",
    "word_tokens_filt = [w for w in word_tokens if (w not in stop_words) and (w not in string.punctuation) and (w.isalnum())]\n",
    "word_tokens_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ec9a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont need car enjoy melbournes food culture scene dont want ride everywhere either something oh wait ebike'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct clean text\n",
    "text_clean = ' '.join(word_tokens_filt).lower()\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a57f485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, augment our text cleanup function\n",
    "\n",
    "# no need to collect stop-words every time we run the function; pass the set as an argument\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleanup(s, stop_words):\n",
    "    s_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', s)))\n",
    "    s_noemoji = s_unesc.encode('ascii', 'ignore').decode('ascii')\n",
    "    # normalize to lowercase and tokenize\n",
    "    wt = word_tokenize(s_noemoji.lower())\n",
    "    \n",
    "    # filter word-tokens\n",
    "    wt_filt = [w for w in wt if (w not in stop_words) and (w not in string.punctuation) and (w.isalnum())]\n",
    "    \n",
    "    # return clean string\n",
    "    return ' '.join(wt_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d12af76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6632a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dont need car enjoy melbournes food culture scene dont want ride everywhere either something oh wait ebike\n"
     ]
    }
   ],
   "source": [
    "text_clean = text_cleanup(text, stop_words)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d633b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment analysis:\n",
    "# install textblob package (simplifies working with nltk)\n",
    "# in terminal:\n",
    "#     conda install -c conda-forge textblob\n",
    "#\n",
    "# alternatively, right here in the notebook (uncomment the next two lines and run):\n",
    "#import sys\n",
    "#!conda install -c conda-forge --yes --prefix {sys.prefix} textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3157695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27c43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob is built upon NLTK and provides an easy interface to the NLTK library\n",
    "# https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library\n",
    "analysis = TextBlob(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d91b1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.4, subjectivity=0.5)\n"
     ]
    }
   ],
   "source": [
    "## polarity: -1 to 1\n",
    "## subjectivity: 0 to 1 (1 is personal opinion, 0 more factual support)\n",
    "print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2558c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a function that takes an input string and displays sentiment analysis;\n",
    "# then come up with some example sentenses to test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "972b47c9-dd3a-4d43-8120-7356e3e338af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit example (note a multi-line string)\n",
    "s = \"\"\"\\\n",
    "Today is day 1 of Schwab usage for me. I hate this app.\\n\n",
    "It feels clunky and you have to swipe screens to see the information that you want,\n",
    "instead of being able to have it all in one place at a glance.\\n\n",
    "Worst of all, there is a massive lag in pricing. TD would keep up in near-realtime.\n",
    "Schwab seems to be using 1998 computers to provide pricing updates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53702be7-f1c9-45bf-8c67-257a06c4a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use your function to evaluate sentiment of the Reddit example post"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
